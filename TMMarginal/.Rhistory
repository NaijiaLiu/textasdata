x1na <- ifelse(I(x1 - x3*x4 - (x2)^3) > quantile( I(x1 - x3*x4 - (x2)^3), cutpoint[i]) , NA, x1)
#x1na <- ifelse(x1 > cutpoint[i] + 0.25 , NA, x1)
print(paste('start with', i, sep = ' '))
sum(is.na(x1na))
#simtrue <- lm(y ~ x1 + x2 + x3 + x4)
simtrue <- lm(y ~ x1 + x2 + x3 + x4 )
simmis <- lm(y ~  x1na + x2 + x3 + x4 )
simresult <- cbind(simtrue$coefficients, simmis$coefficients)
colnames(simresult) <- c("true", "missing")
simresult
x1naimput <- ifelse(x1na == "NA", mean(x1na), x1na)
hist(x1naimput)
simmisimput <- lm(y ~  x1naimput + x2 + x3 +  x4  )
simresult <- cbind(simresult, simmisimput$coefficients)
colnames(simresult) <- c("true", "missing", "impute")
simresult
##############################mice impute####################
library(mice)
dataset <- cbind(x1na, x2, x3, x4, y, x5,x6,x7,x8,x9,x10)
mice <- mice::mice(dataset, method = 'cart', seed=320 )
mice$imp$x1na
datasetmice <- complete(mice)
simmismice <- lm(datasetmice$y ~  datasetmice$x1na +datasetmice$x2 + datasetmice$x3 +   datasetmice$x4, data = datasetmice)
simresult <- cbind(simresult, simmismice$coefficients)
colnames(simresult) <- c("true", "missing", "impute","mice")
simresult
######################################amelia#################
library(Amelia)
dataset <- cbind(x1na, x2, x3, x4, y, x5, x6, x7, x8, x9, x10)
ameliaset <- Amelia::amelia(dataset, m=5)
#simmisamelia <- lapply(ameliaset$imputations, function(i) lm(y ~ x1na + x2+x3+x4,  data = i))
ameliacoef <- matrix(ncol =5, nrow = 5)
for(imp in 1:5){
datadata <- as.data.frame(ameliaset$imputations[imp])
colnames(datadata) <- c("x1na", "x2", "x3", "x4", "y", "x5", "x6", "x7", "x8", "x9", 'x10')
simamelia <- lm(datadata$y ~ datadata$x1na +datadata$x2 + datadata$x3 +
datadata$x4  )
ameliacoef[imp,] <- simamelia$coefficients
}
simmisamelia <- apply(ameliacoef,2,mean )
################################################DML?
dataset <- as.data.frame(cbind(x1na, x2, x3, x4, y ))
set.seed(99)
numbers <- sample(1:nrow(dataset), 0.5*nrow(dataset), replace = FALSE)
test <- dataset[numbers,]
train <- dataset[-numbers,]
sum(is.na(train$x1na))
sum(is.na(test$x1na))
train$treat <- ifelse(is.na(train$x1na), 1, 0)
table(train$treat)
train$x1na <- ifelse(is.na(train$x1na), 0, train$x1na)
test$treat <- ifelse(is.na(test$x1na), 1, 0)
table(test$treat)
test$x1na <- ifelse(is.na(test$x1na), 0, test$x1na)
###on the train data only
#predict y on x, t
trainy <- randomForest::randomForest(train$y~.-x1na, data=train, type="regression")
#predict t on x
#trainT <- glm(train$treat~ .-x1na, data=train, family =gaussian)
trainT <- randomForest::randomForest(as.factor(train$treat) ~. -x1na, data=train, type='classification')
#use the model on test data
test_predict_y <- predict(trainy, newdata=test)
test_predict_t <- predict(trainT, newdata=test)
# regress the errors to get theta1
test_error_y <- test$y -test_predict_y
test_error_t <- test$treat - as.numeric(test_predict_t)
regress1 <- lm( test_error_y ~ test_error_t)
theta1 <- regress1$coefficients[2]
theta1
###on the test data only
#predict y on x, t
testy <- randomForest::randomForest(test$y~.-x1na, data=test,type="regression")
#predict t on x
#testT <- glm(test$treat~.-x1na, data=test, family = gaussian)
testT <- randomForest::randomForest(as.factor(test$treat )~. -x1na, data=test,  type="classification")
#use the model on train data
train_predict_y <- predict(testy, newdata = train)
train_predict_t <- predict(testT, newdata = train)
# regress the errors to get theta1
train_error_y <- train$y - train_predict_y
train_error_t <- train$treat - as.numeric(train_predict_t)
regress2 <- lm(train_error_y ~ train_error_t)
theta2 <- regress2$coefficients[2]
theta2
#####################done w/ DML procedure
theta <- (theta1 + theta2)/2
theta
############################################################
#dataset$newy <- ifelse(is.na(dataset$x1na), dataset$y - theta, dataset$y)
dataset$newx1 <- ifelse(is.na(dataset$x1na), mean(x1na, na.rm=TRUE)
,dataset$x1na)
dataset$treat <- ifelse(is.na(dataset$x1na),theta,0)
#dataset$treat2 <- ifelse(is.na(dataset$x1na), theta_2, 0)
simdml <- lm(y ~ newx1 +treat + x2 + x3 + x4 , data=dataset)
simdml$coefficients
simresult <- as.data.frame(simresult)
simresult[6,] <- c(NA, NA, NA, NA)
simresult <- cbind(simresult, c(simdml$coefficients[-3], simdml$coefficients[3]))
colnames(simresult) <- c("true", "missing", "impute","mice", "DML")
rownames(simresult) <- c('Intercept', 'x1', 'x2', 'x3', 'x4' ,#"x5", "x6", "x7", "x8", "x9", 'x10',
'T_x1')
simresult
theta
errordml[i,] <- simresult$DML - c(correct,0)
errormice[i,] <- simresult$mice[1:5] - correct
erroramelia[i,] <- simmisamelia - correct
preddml[i] <- mean(predict(simdml, newdata = dataset)- y)^2
predmice[i] <- mean(predict(simmismice, newdata=datasetmice) - y)^2
}
#ggplot(errordml, aes(x = cutpoint)) +
# geom_line(aes(y = errordml$V2, colour = "x1")) +
### geom_line(aes(y = errordml$V1, colour = "Intercept")) +
#geom_line(aes(y = errordml$V3, colour = "x2")) +
#geom_line(aes(y = errordml$V4, colour = "x3")) +
#geom_line(aes(y = errordml$V5, colour = "x4"))
#ggplot(errormice, aes(x=cutpoint)) +
## #geom_line(aes(y=errormice$V1 , colour = 'Intercept')) +
#geom_line(aes(y=errormice$V2 , colour = 'x1')) +
#geom_line(aes(y=errormice$V3 , colour = 'x2')) +
#geom_line(aes(y=errormice$V4 , colour = 'x3')) +
#geom_line(aes(y=errormice$V5 , colour = 'x4'))
errordml <- as.data.frame(errordml)
errormice <- as.data.frame(errormice)
erroramelia <- as.data.frame(erroramelia)
errors <- cbind(errormice, erroramelia, errordml)
colnames(errors) <- c('mice0','mice1','mice2',
'mice3','mice4',#"mice5", "mice6", "mice7", "mice8", "mice9", "mice10",
"ame0", "ame1", "ame2", "ame3", "ame4",
#"ame5","ame6","ame7","ame8","ame9", "ame10",
"dml0","dml1","dml2","dml3","dml4" #,"dml5", "dml6", "dml7", "dml8", "dml9", 'dml10'
,"dmlT")
#head(errors)
#pred <- cbind(preddml, predmice)
#pred <- as.data.frame(pred)
#cutpoint <- cutpoint[1:25]
library(ggplot2)
intercept <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice0 , colour = 'mice-cart')) +
geom_line(aes(y=errors$dml0 , colour = 'dml')) +
geom_line(aes(y=errors$ame0 , colour = 'amelia')) +
scale_color_discrete(name="Method") +
xlab('percentage non-missing') +
ylab('Intercept') +
geom_hline(yintercept = 0, alpha=0.6)
V1 <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice1 , colour = 'mice-cart')) +
geom_line(aes(y=errors$dml1 , colour = 'dml')) +
geom_line(aes(y=errors$ame1 , colour = 'amelia')) +
scale_color_discrete(name="Method") +
xlab('percentage non-missing') +
ylab('x1 coef') +
geom_hline(yintercept = 0, alpha=0.6)
V2 <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice2 , colour = 'mice-cart')) +
geom_line(aes(y=errors$dml2 , colour = 'dml')) +
geom_line(aes(y=errors$ame2 , colour = 'amelia')) +
scale_color_discrete(name="Method") +
xlab('percentage missing') +
ylab('x2 coef') +
geom_hline(yintercept = 0, alpha=0.6)
V3 <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice3 , colour = 'mice-cart')) +
geom_line(aes(y=errors$dml3, colour = 'dml')) +
geom_line(aes(y=errors$ame3 , colour = 'amelia')) +
scale_color_discrete(name="Method") +
xlab('percentage non-missing') +
ylab('x3 coef') +
geom_hline(yintercept = 0, alpha=0.6)
V4 <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice4 , colour = 'mice-cart')) +
scale_color_discrete(name="Method") +
geom_line(aes(y=errors$dml4 , colour = 'dml')) +
geom_line(aes(y=errors$ame4 , colour = 'amelia')) +
xlab('percentagenon- missing') +
ylab('x4 coef') +
geom_hline(yintercept = 0, alpha=0.6)
#predict <- ggplot(data=pred, aes(x=cutpoint)) +
# geom_line(aes(y=pred$predmice , colour = 'mice-cart')) +  scale_color_discrete(name="Method") +
#  geom_line(aes(y=pred$preddml , colour = 'dml')) +
#  xlab('percentage non-missing') +
#  ylab('Prediction Error') +
#  geom_hline(yintercept = 0, alpha=0.6)
#grid.arrange(intercept, V1, V2, V3, V4, nrow = 2)
#grouped <- grid.arrange(arrangeGrob(intercept + theme(legend.position="none"),
# V1 + theme(legend.position="none"),
# V2 + theme(legend.position="none"),
# V3 + theme(legend.position="none"),
# V4 + theme(legend.position="none"),
# predict+ theme(legend.position="none"),
# nrow=2), V1$labels)
library(grid)
library(gridExtra)
grid_arrange_shared_legend(intercept + theme(legend.position="none"),
V1 + theme(legend.position="none"),
V2 + theme(legend.position="none"),
V3 + theme(legend.position="none"),
V4 + theme(legend.position="none"),
#  predict+ theme(legend.position="none"),
ncol = 3, nrow = 2, position = "bottom")
#amelia <- ggplot(data=errors, aes(x=cutpoint)) +
#  geom_line(aes(y=errors$ame0 , colour = 'Intercept')) +
#  geom_line(aes(y=errors$ame1 , colour = 'x1')) +
#  geom_line(aes(y=errors$ame2+0.3 , colour = 'x2')) +
#  geom_line(aes(y=errors$ame3 , colour = 'x3')) +
#  geom_line(aes(y=errors$ame4 , colour = "x4")) +
#  scale_color_discrete(name="variable") +
#  xlab('percentage non-missing') +
#  ylab('Error (Amelia)') +
#  ylim(NA, 8)+
#  geom_hline(yintercept = 0, alpha=0.6)
set.seed(321)
#n <- 1000
n <- 1000
x3 <- rnorm(n, 0, 5)
#x1 <- rnorm(n, 0, 5)
x1 <- x3 +
rbinom(n, 100, 0.2)
#x2 <- rpois(n, 10)
x2 <- rnorm(n, 5, 10) - rpois(n, 10)
#x3 <- x1+ rbinom(n, 100, 0.2)
x4 <- (rnorm(n, 20, 20))^2
#x4 <- (x4)^2
error <- rnorm(n, 0, 2)
########################
x2 <- (x2-3)^3
x4 <- sin(x4)
x3 <- x1*x3
#######################
x5 <- rnorm(n, 15, 40)
x6 <- rpois(n, 5)
x7 <-   rbinom(n, 50, 0.5)
x8 <-  x5*x6
x9 <-   rnorm(n, 0, 1)
x10 <-  rnorm(n, 0, 1)
######################
#y <- 5 + 20*x1 + 40*x2+ 60*x3 + 100*x4 + x5+x6+x7+x8+x9+x10+error
y <- 5 + 20*x1 + 40*x2+ 60*x3 + 100*x4 +error
correct <- c(5, 20, 40 ,60, 100)
cutpoint <- seq(0.80, 0.92, by =0.01)
#cutpoint <- seq(0.85, 0.90, by =0.01)
length(cutpoint)
errordml <- matrix(nrow = length(cutpoint), ncol = length(correct)+1)
errormice <- matrix(nrow = length(cutpoint), ncol = length(correct))
erroramelia <- matrix(nrow = length(cutpoint), ncol= length(correct))
preddml <- NULL
predmice <- NULL
simresult <- NULL
for(i in 1:length(cutpoint))  {
#x1na <- ifelse(x1 >quantile(x3-21, cutpoint[i]) , NA, x1)
x1na <- ifelse(y > quantile( y, cutpoint[i]) , NA, x1)
x1na <- ifelse(I(x1 - x3*x4 - (x2)^3) > quantile( I(x1 - x3*x4 - (x2)^3), cutpoint[i]) , NA, x1)
#x1na <- ifelse(x1 > cutpoint[i] + 0.25 , NA, x1)
print(paste('start with', i, sep = ' '))
sum(is.na(x1na))
#simtrue <- lm(y ~ x1 + x2 + x3 + x4)
simtrue <- lm(y ~ x1 + x2 + x3 + x4 )
simmis <- lm(y ~  x1na + x2 + x3 + x4 )
simresult <- cbind(simtrue$coefficients, simmis$coefficients)
colnames(simresult) <- c("true", "missing")
simresult
x1naimput <- ifelse(x1na == "NA", mean(x1na), x1na)
hist(x1naimput)
simmisimput <- lm(y ~  x1naimput + x2 + x3 +  x4  )
simresult <- cbind(simresult, simmisimput$coefficients)
colnames(simresult) <- c("true", "missing", "impute")
simresult
##############################mice impute####################
library(mice)
dataset <- cbind(x1na, x2, x3, x4, y, x5,x6,x7,x8,x9,x10)
mice <- mice::mice(dataset, method = 'cart', seed=320 )
mice$imp$x1na
datasetmice <- complete(mice)
simmismice <- lm(datasetmice$y ~  datasetmice$x1na +datasetmice$x2 + datasetmice$x3 +   datasetmice$x4, data = datasetmice)
simresult <- cbind(simresult, simmismice$coefficients)
colnames(simresult) <- c("true", "missing", "impute","mice")
simresult
######################################amelia#################
library(Amelia)
dataset <- cbind(x1na, x2, x3, x4, y, x5, x6, x7, x8, x9, x10)
ameliaset <- Amelia::amelia(dataset, m=5)
#simmisamelia <- lapply(ameliaset$imputations, function(i) lm(y ~ x1na + x2+x3+x4,  data = i))
ameliacoef <- matrix(ncol =5, nrow = 5)
for(imp in 1:5){
datadata <- as.data.frame(ameliaset$imputations[imp])
colnames(datadata) <- c("x1na", "x2", "x3", "x4", "y", "x5", "x6", "x7", "x8", "x9", 'x10')
simamelia <- lm(datadata$y ~ datadata$x1na +datadata$x2 + datadata$x3 +
datadata$x4  )
ameliacoef[imp,] <- simamelia$coefficients
}
simmisamelia <- apply(ameliacoef,2,mean )
################################################DML?
dataset <- as.data.frame(cbind(x1na, x2, x3, x4, y ))
set.seed(99)
numbers <- sample(1:nrow(dataset), 0.5*nrow(dataset), replace = FALSE)
test <- dataset[numbers,]
train <- dataset[-numbers,]
sum(is.na(train$x1na))
sum(is.na(test$x1na))
train$treat <- ifelse(is.na(train$x1na), 1, 0)
table(train$treat)
train$x1na <- ifelse(is.na(train$x1na), 0, train$x1na)
test$treat <- ifelse(is.na(test$x1na), 1, 0)
table(test$treat)
test$x1na <- ifelse(is.na(test$x1na), 0, test$x1na)
###on the train data only
#predict y on x, t
trainy <- randomForest::randomForest(train$y~.-x1na, data=train, type="regression")
#predict t on x
#trainT <- glm(train$treat~ .-x1na, data=train, family =gaussian)
trainT <- randomForest::randomForest(as.factor(train$treat) ~. -x1na, data=train, type='classification')
#use the model on test data
test_predict_y <- predict(trainy, newdata=test)
test_predict_t <- predict(trainT, newdata=test)
# regress the errors to get theta1
test_error_y <- test$y -test_predict_y
test_error_t <- test$treat - as.numeric(test_predict_t)
regress1 <- lm( test_error_y ~ test_error_t)
theta1 <- regress1$coefficients[2]
theta1
###on the test data only
#predict y on x, t
testy <- randomForest::randomForest(test$y~.-x1na, data=test,type="regression")
#predict t on x
#testT <- glm(test$treat~.-x1na, data=test, family = gaussian)
testT <- randomForest::randomForest(as.factor(test$treat )~. -x1na, data=test,  type="classification")
#use the model on train data
train_predict_y <- predict(testy, newdata = train)
train_predict_t <- predict(testT, newdata = train)
# regress the errors to get theta1
train_error_y <- train$y - train_predict_y
train_error_t <- train$treat - as.numeric(train_predict_t)
regress2 <- lm(train_error_y ~ train_error_t)
theta2 <- regress2$coefficients[2]
theta2
#####################done w/ DML procedure
theta <- (theta1 + theta2)/2
theta
############################################################
#dataset$newy <- ifelse(is.na(dataset$x1na), dataset$y - theta, dataset$y)
dataset$newx1 <- ifelse(is.na(dataset$x1na), mean(x1na, na.rm=TRUE)
,dataset$x1na)
dataset$treat <- ifelse(is.na(dataset$x1na),theta,0)
#dataset$treat2 <- ifelse(is.na(dataset$x1na), theta_2, 0)
simdml <- lm(y ~ newx1 +treat + x2 + x3 + x4 , data=dataset)
simdml$coefficients
simresult <- as.data.frame(simresult)
simresult[6,] <- c(NA, NA, NA, NA)
simresult <- cbind(simresult, c(simdml$coefficients[-3], simdml$coefficients[3]))
colnames(simresult) <- c("true", "missing", "impute","mice", "DML")
rownames(simresult) <- c('Intercept', 'x1', 'x2', 'x3', 'x4' ,#"x5", "x6", "x7", "x8", "x9", 'x10',
'T_x1')
simresult
theta
errordml[i,] <- simresult$DML - c(correct,0)
errormice[i,] <- simresult$mice[1:5] - correct
erroramelia[i,] <- simmisamelia - correct
preddml[i] <- mean(predict(simdml, newdata = dataset)- y)^2
predmice[i] <- mean(predict(simmismice, newdata=datasetmice) - y)^2
}
#ggplot(errordml, aes(x = cutpoint)) +
# geom_line(aes(y = errordml$V2, colour = "x1")) +
### geom_line(aes(y = errordml$V1, colour = "Intercept")) +
#geom_line(aes(y = errordml$V3, colour = "x2")) +
#geom_line(aes(y = errordml$V4, colour = "x3")) +
#geom_line(aes(y = errordml$V5, colour = "x4"))
#ggplot(errormice, aes(x=cutpoint)) +
## #geom_line(aes(y=errormice$V1 , colour = 'Intercept')) +
#geom_line(aes(y=errormice$V2 , colour = 'x1')) +
#geom_line(aes(y=errormice$V3 , colour = 'x2')) +
#geom_line(aes(y=errormice$V4 , colour = 'x3')) +
#geom_line(aes(y=errormice$V5 , colour = 'x4'))
errordml <- as.data.frame(errordml)
errormice <- as.data.frame(errormice)
erroramelia <- as.data.frame(erroramelia)
errors <- cbind(errormice, erroramelia, errordml)
colnames(errors) <- c('mice0','mice1','mice2',
'mice3','mice4',#"mice5", "mice6", "mice7", "mice8", "mice9", "mice10",
"ame0", "ame1", "ame2", "ame3", "ame4",
#"ame5","ame6","ame7","ame8","ame9", "ame10",
"dml0","dml1","dml2","dml3","dml4" #,"dml5", "dml6", "dml7", "dml8", "dml9", 'dml10'
,"dmlT")
#head(errors)
#pred <- cbind(preddml, predmice)
#pred <- as.data.frame(pred)
#cutpoint <- cutpoint[1:25]
library(ggplot2)
intercept <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice0 , colour = 'mice-cart')) +
geom_line(aes(y=errors$dml0 , colour = 'dml')) +
geom_line(aes(y=errors$ame0 , colour = 'amelia')) +
scale_color_discrete(name="Method") +
xlab('percentage non-missing') +
ylab('Intercept') +
geom_hline(yintercept = 0, alpha=0.6)
V1 <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice1 , colour = 'mice-cart')) +
geom_line(aes(y=errors$dml1 , colour = 'dml')) +
geom_line(aes(y=errors$ame1 , colour = 'amelia')) +
scale_color_discrete(name="Method") +
xlab('percentage non-missing') +
ylab('x1 coef') +
geom_hline(yintercept = 0, alpha=0.6)
V2 <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice2 , colour = 'mice-cart')) +
geom_line(aes(y=errors$dml2 , colour = 'dml')) +
geom_line(aes(y=errors$ame2 , colour = 'amelia')) +
scale_color_discrete(name="Method") +
xlab('percentage missing') +
ylab('x2 coef') +
geom_hline(yintercept = 0, alpha=0.6)
V3 <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice3 , colour = 'mice-cart')) +
geom_line(aes(y=errors$dml3, colour = 'dml')) +
geom_line(aes(y=errors$ame3 , colour = 'amelia')) +
scale_color_discrete(name="Method") +
xlab('percentage non-missing') +
ylab('x3 coef') +
geom_hline(yintercept = 0, alpha=0.6)
V4 <- ggplot(data=errors, aes(x=cutpoint)) +
geom_line(aes(y=errors$mice4 , colour = 'mice-cart')) +
scale_color_discrete(name="Method") +
geom_line(aes(y=errors$dml4 , colour = 'dml')) +
geom_line(aes(y=errors$ame4 , colour = 'amelia')) +
xlab('percentagenon- missing') +
ylab('x4 coef') +
geom_hline(yintercept = 0, alpha=0.6)
#predict <- ggplot(data=pred, aes(x=cutpoint)) +
# geom_line(aes(y=pred$predmice , colour = 'mice-cart')) +  scale_color_discrete(name="Method") +
#  geom_line(aes(y=pred$preddml , colour = 'dml')) +
#  xlab('percentage non-missing') +
#  ylab('Prediction Error') +
#  geom_hline(yintercept = 0, alpha=0.6)
#grid.arrange(intercept, V1, V2, V3, V4, nrow = 2)
#grouped <- grid.arrange(arrangeGrob(intercept + theme(legend.position="none"),
# V1 + theme(legend.position="none"),
# V2 + theme(legend.position="none"),
# V3 + theme(legend.position="none"),
# V4 + theme(legend.position="none"),
# predict+ theme(legend.position="none"),
# nrow=2), V1$labels)
library(grid)
library(gridExtra)
grid_arrange_shared_legend(intercept + theme(legend.position="none"),
V1 + theme(legend.position="none"),
V2 + theme(legend.position="none"),
V3 + theme(legend.position="none"),
V4 + theme(legend.position="none"),
#  predict+ theme(legend.position="none"),
ncol = 3, nrow = 2, position = "bottom")
#amelia <- ggplot(data=errors, aes(x=cutpoint)) +
#  geom_line(aes(y=errors$ame0 , colour = 'Intercept')) +
#  geom_line(aes(y=errors$ame1 , colour = 'x1')) +
#  geom_line(aes(y=errors$ame2+0.3 , colour = 'x2')) +
#  geom_line(aes(y=errors$ame3 , colour = 'x3')) +
#  geom_line(aes(y=errors$ame4 , colour = "x4")) +
#  scale_color_discrete(name="variable") +
#  xlab('percentage non-missing') +
#  ylab('Error (Amelia)') +
#  ylim(NA, 8)+
#  geom_hline(yintercept = 0, alpha=0.6)
pdf('./Google Drive/Spring2018/POL574/plot/KennedyOConnorSingleFigure.pdf',20,30)
par(mar=c(5,13,3,1))
plot(0,0,type="n",xlim=c(-3,3),ylim=c(0,Ntopics),xlab="",ylab="",main='',axes=FALSE)
# axis(1)
axis(2,at=1:Ntopics,labels=LDATopicLabels[KennedyOConnorOrder],las=1)
for (i in 1:Ntopics){
lines(c(-10,10),c(i,i),col="lightgrey")
points(-KennedyOConnorMedian[KennedyOConnorOrder[i]],i,pch=16,cex=1.2)
lines(c(quantile(-KennedyOConnor[KennedyOConnorOrder[i],], probs=c(.05)),quantile(-KennedyOConnor[KennedyOConnorOrder[i],], probs=c(.95))), c(i,i),lwd=2)
}
lines(c(0,0), c(1,Ntopics), lty=5, col='grey')
axis(side=1)
mtext(side=3, line = 0, at=-2, 'Kennedy More Liberal', cex=1.5)
mtext(side=3, line = 0, at=2, 'Kennedy More Conservative', cex=1.5)
mtext(side=1,line=2,'Posterior Difference Between Kennedy and O\'Connor', cex=1)
dev.off()
set.seed(3829)
x1 <- rnorm(1000)
y <- 1 + 0.5 * x1 + rnorm(1000)
dat <- data.frame(x1, y)
out1 <- lm(dat$y ~ dat$x1)
out2 <- lm(y ~ x1, data = dat)
x1 <- rnorm(100)
pd1 <- predict(out1, newdata = dat)
pd2 <- predict(out2, newdata = dat)
head(pd1)
head(pd2)
nrows(pd1)
nrows(pd2)
dim(pd2)
length(pd2)
length(pd1)
datatata <- as.data.frame(x1)
pd1 <- predict(out1, newdata = datatata)
pd2 <- predict(out2, newdata = datatata)
names(datatata)
library(devtools)
setwd("Google Drive/Fall2017/591/NL_Draft/Package/TMMarginal/")
library(devtools)
devtools::build()
devtools::load_all()
devtools::document()
?Marginalikelihood
?visualizeTMM
converage_dml <- readRDS("../../../../../../../naijialiu/Google Drive/Spring2018/ReadingClass/converage_dml.rds")
converage_dml <- readRDS("../../../../../../../naijialiu/Google Drive/Spring2018/ReadingClass/converage_dml)
converage_dml <- readRDS("../../../../../../../naijialiu/Google Drive/Spring2018/ReadingClass/converage_dml")
converage_dml <- readRDS("../../../../../../../naijialiu/Google Drive/Spring2018/ReadingClass/converage_dml.rds")
rowMeans(converage_dml)
converage_dml
dim(converage_dml)
View(converage_dml)
